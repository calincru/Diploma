\chapter{Evaluation}\label{chapter:eval}

In this chapter we summarize the key results we reached by running SymNet on
various models generated by our tool, \TOOL.  It is divided in two sections.
In the first one we argue that the models we build capture the packet
processing logic behind netfilter.  In the second one we use models built from
synthetic rules to evaluate the quality of our code generation approach
discussed in previous chapters.


\section{Acceptance tests}

Acceptance tests help answer the question \emph{"Does our model reflect the
semantics of netfilter, the real system?"}  Certainly we would rather not
introduce inconsistencies in a software system that is supposed to find bugs in
another one. In some formal verification systems the \emph{model - real system}
equivalence is enforced by construction.  However, this is not true in SymNet
for reasons described in \labelindexref{Section}{sec:symnet-sefl}, and, thus,
we need to run hand-crafted tests to confirm our expectations.

Driven by this insight from the very beginning, we have performed extensive
unit testing, focusing on specific components and/or behaviours, as well as
integration testing, when linking them together.  Our testing suite contains
more than \textbf{130 total tests} and achieves over \textbf{91\% code
coverage}~\cite{github-repo}.  This means that we have generated models that
cover most of our model generation corner cases.  However, this should not be
confused with \emph{generated code} coverage, which depends on input packet,
input port, as well as the interaction with the network as a whole, considering
that some of the networks elements that we model are stateful.

In the following paragraphs we show a small subset of the scenarios that we
tested against.

\paragraph{Simple NAT policy.}
We start with a very simple example to give an intuition of how these tests are
performed by including the Scala code too.

\labelindexref{Listing}{lst:unreachable-rule} shows the entire test.  Between
\textbf{lines 2-7} the nat/POSTROUTING chain is set up with two rules: one to
match an entire private network (\lstinline{-s 192.168.1.0/24}) and the other
one to only match a host in the  same network (\lstinline{-s 192.168.1.100}).
Between \textbf{lines 8-13} we run symbolic execution injecting a packet with
the source IP set to \lstinline{192.168.1.100} (line 12), starting from this
chain's input port (lines 10-11).  Between \textbf{lines 14-19} we define the
\emph{rewrite constrain} that we expect to take place.  Finally, at line 22 we
\textbf{express the policy} using the custom Scala matcher,
\emph{containConstrain} that looks up the given constrain in the list of
successful paths output by symbolic execution.

\begin{listing}[H]
  \caption{An example of a NAT misconfiguration taken from a \emph{Local Area
  Networks} lecture quick.  Notice that Scala already makes policy
  specification easy to understand owing to its relaxed syntax rules.}
  \label{lst:unreachable-rule}
  \sourcecode{scala}{src/code/simple-nat.scala}
\end{listing}

This example is intended to show how a network administrator would express
policies and catch inconsistencies.  Indeed, this test fails because the
specified host IP matches the first rule in the table.  In fact, the second
rule will never be matched, indicating a misconfiguration which would hopefully
be found as a result of the policy failing.

\paragraph{Conntrack state switch.}
TBA

\paragraph{Source NATed connection.}
TBA

\paragraph{Untracked connection.}
TBA

\bigskip

An alternative approach to bypassing the \emph{model - real system} equivalence
problem that we have yet to experiment with is \emph{black-box testing}.  It
requires establishing a(n) (usually large) list of input packets and injecting
them both in the real system and in its model constructed with \TOOL.  Then,
monitor output packets in the real iptables-enabled device and try to match
them against the exhaustive list of (symbolic) packet flows output by SymNet.
If for some input packet there is no symbolic flow that matches the observed
output packet, the equivalence is denied.  The inverse is obviously not
necessarily true, but it increases our \emph{belief} that the implementation is
correct.


\section{Performance tests}


Things to say:
* tests performed:
1. many rules, on a single filter table;
2. many IPTRouters in line.
* overall results
* framework used, benchmark, etc
* tables/graphs

\bigskip

Besides these script-generated configurations, we have also run SymNet on a
model of an OpenStack qrouter from a real deployment.  Its complete iptables
dump is included in \labelindexref{Appendix}{app:qrouter}.

Exploring all possible execution paths by inserting a pure symbolic packet to
one of its input ports takes around \textbf{25s} and \textbf{6GB} of RAM.  This
generates 1345 packet flows.  Only 74 of them are successful.  Out of all
failed ones only 6 are explicitly dropped.  The remaining ones are packets
caused by internal logic of our model that cannot be materialized.  Since there
are only two rules that DROP packets, it becomes manageable to inspect those
packets by hand.  Moreover, if we constrain just the destination IP address of
the injected packet to a concrete value, symbolic execution time drops to
\textbf{5s}.  This shows the importance of having a policy drive symbolic
execution rather than exploring all possible execution outcomes.
